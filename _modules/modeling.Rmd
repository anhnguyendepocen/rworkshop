---
layout: module
title: Modeling
date: 2018-01-01 00:00:08
category: module
links:
  script: modeling.R
  data: els_plans_2.dta
output:
  md_document:
    variant: markdown_mmd
    preserve_yaml: true
---

```{r, include = FALSE, purl = FALSE}
source('knit_setup.R')
```
```{r, include = FALSE, purl = TRUE}
################################################################################
##
## <PROJ> R Workshop
## <FILE> modeling.R 
## <INIT> 16 January 2018
## <AUTH> Benjamin Skinner (GitHub/Twitter: @btskinner)
##
################################################################################

## clear memory
rm(list = ls())

## libraries
library(tidyverse)
library(haven)

```
```{r, echo = FALSE, purl = FALSE, warnings = F, messages = F}
suppressMessages(library(tidyverse))
library(haven)

```

After your data have been wrangled from raw values to an analysis data
set and you've explored it with summary statistics and graphics, you
are ready to model it and make inferences. As one should expect from a
statistical language, R has a powerful system for fitting statistical
and econometric models.

In this module we'll still use the ELS plans data set, but we'll use
one that has been tidied up a bit. Since the point of this
module is to show the structure of running, say, an OLS regression in
R, little weight should be given to the results. With that caveat,
let's load the libraries and data!

```r
## libraries
library(tidyverse)
library(haven)

```

```{r}
## read in data
df <- read_dta('../data/els_plans_2.dta')

```

# t-test
```{r, echo = FALSE, purl = TRUE}
## ---------------------------
## t-test
## ---------------------------

```

One common statistical test is a t-test for a difference in means
across groups (there are, of course, [others and R can compute
them](https://www.rdocumentation.org/packages/stats/versions/3.4.3/topics/t.test)). This
version of the test can be computed using the R formula syntax: `y ~
x`. In our example, we'll compute base-year math scores against
mother's college education level. Notice that since we have the `data
= df` argument after the comma, we don't need to include `df$` before
the two variables.

```{r}
## t-test of difference in math scores across mother education (BA/BA or not)
t.test(bynels2m ~ moth_ba, data = df, var.equal = TRUE)

```

```{r, include = FALSE, purl = FALSE}
## t-test of difference in math scores across mother education (BA/BA or not)
ttest <- t.test(bynels2m ~ moth_ba, data = df, var.equal = TRUE)

```

> #### Quick exercise
> Run a t-test of reading scores against whether the father has a
> Bachelor's degree (`fath_ba`).

# Linear model
```{r, echo = FALSE, purl = TRUE}
## ---------------------------
## linear model
## ---------------------------

```
```{r, echo = FALSE, purl = TRUE}
## ------------
## ttest
## ------------

```
Linear models are the go-to method of making inferences for many data
analysts. In R, the `lm()` command is used to compute an OLS
regression. Unlike above, where we just let the `t.test()` output
print to the console, we can and will store the output in an object.

First, let's compute the same t-test but in a regression
framework. Because we assumed equal variances between the
distributions in the t-test above (`var.equal = TRUE`), we should get
the same results as we did before.

```{r}
## compute same test as above, but in a linear model
fit <- lm(bynels2m ~ moth_ba, data = df)
fit

```

The output is a little thin: just the coefficients. To see the full
range of information you want from regression output, use the
`summary()` function wrapped around the `fit` object.

```{r}
## use summary to see more information about regression
summary(fit)

```

Looks like the coefficient on `moth_ed`, ```r fit$coefficients[[2]]```,
is the same as the difference between the groups in the ttest, ```r 
ttest$estimate[[2]] - ttest$estimate[[1]]```, and the test statistics are
the same value: ```r ttest$statistic```. Success!

## Multiple regression
```{r, echo = FALSE, purl = TRUE}
## ------------
## lm w/terms
## ------------

```

To fit a multiple regression, use the same formula framework that
we've use before with the addition of all the terms you want on
right-hand side of the equation separated by plus (`+`) signs.

```{r}
## linear model with more than one covariate on the RHS
fit <- lm(bynels2m ~ byses1 + female + moth_ba + fath_ba + lowinc,
          data = df)
summary(fit)

```

The full output tells you:

* the model that you fit, under `Call:`
* a table of coefficients with
  * the main estimate (`Estimate`)
  * the estimate error (`Std. Error`)
  * the test statistic (`t value` with this model)
  * the p value (`Pr(>|t|`)
* significance stars (`.` and `*`) along with legend
* the R-squared values (`Multiple R-squared` and `Adjusted
    R-squared`)
* the model F-statistic (`F-statistic`)	
* number of observations dropped if any

If observations were dropped, you can recover the number of
observations used with the `nobs()` function.

```{r}
## check number of observations
nobs(fit)

```

The `fit` object also holds a lot of other information that is
sometimes useful.

```{r}
## see what fit object holds
names(fit)

```

For example, both `fitted.values` and `residuals` are stored in the
object. You can access these "hidden" attributes by treating the `fit`
object like a data frame and using the `$` notation.

```{r}
## see first few fitted values and residuals
head(fit$fitted.values)
head(fit$residuals)

```

> #### Quick exercise
> Add the fitted values to the residuals and store in an object
> (`x`). Compare these values to the math scores in the data frame.

As a final note, the model matrix used fit the regression can be
retrieved using `model.matrix()`. Since we have a lot of observations,
we'll just look at the first few rows.

```{r}
## see the design matrix
head(model.matrix(fit))

```

What this shows is that the fit object actually stores a copy of the
data used to run it. That's really convenient if you want to save the
object to disk (with the `save()` function) so you can review the
regression results later. But keep in mind that if you share that
file, you are sharing the part of the data used to estimate it.

## Using categorical variables or factors
```{r, echo = FALSE, purl = TRUE}
## ------------
## factors
## ------------

```

It's not necessary to pre-construct dummy variables if you want to use
a categorical variable in your model. Instead you can use the
categorical variable wrapped in the `factor()` function. This tells R
that the underlying variable shouldn't be treated as a continuous
value, but should be discrete groups. R will make the dummy variables
on the fly when fitting the model. We'll include the categorical
variable `byrace` in this model.

```{r}
## add factors
fit <- lm(bynels2m ~ byses1 + female + moth_ba + fath_ba + lowinc
          + factor(byrace),
          data = df)
summary(fit)

```

If you're using labeled data like we have been for the past couple of
modules, you can use the `as_factor()` function from
the
[haven library](https://www.rdocumentation.org/packages/haven/versions/1.1.0/topics/as_factor) in
place of the base `factor()` function. You'll still see the
`as_factor(<var>)` prefix on each coefficient, but now you'll have
labels instead of the underlying values, which should make parsing the
output a little easier.

```{r}
## same model, but use as_factor() instead of factor() to use labels
fit <- lm(bynels2m ~ byses1 + female + moth_ba + fath_ba + lowinc
          + as_factor(byrace),
          data = df)
summary(fit)

```

If you look at the model matrix, you can see how R created the dummy
variables from `byrace`.

```{r}
## see what R did under the hood to convert categorical to dummies
head(model.matrix(fit))

```

> #### Quick exercise
> Add the categorical variable `byincome` to the model above. Next use
> `model.matrix()` to check the RHS matrix.

## Interactions
```{r, echo = FALSE, purl = TRUE}
## ------------
## interactions
## ------------

```

Add interactions to a regression using an asterisks (`*`) between the
terms you want to interact. This will add both main terms and the
interaction(s) between the two to the model. Any interaction terms
will be labeled using the base name or factor name of each term
joined by a colon (`:`).


```{r}
## add interactions
fit <- lm(bynels2m ~ byses1*lowinc + factor(bypared)*lowinc, data = df)
summary(fit)

```

## Polynomials
```{r, echo = FALSE, purl = TRUE}
## ------------
## polynomials
## ------------

```

To add quadratic and other polynomial terms to the model, use the
`I()` function, which lets you raise the term to the power you want
in the regression using the caret (`^`) operator. 

In the model below, we add both quadratic and cubic versions of the
reading score to the right-hand side.

```{r}
## add polynomials
fit <- lm(bynels2m ~ bynels2r + I(bynels2r^2) + I(bynels2r^3), data = df)
summary(fit)

```

> #### Quick exercise
> Fit a linear model with both interactions and a polynomial term. Then
> look at the model matrix to see what R did under the hood.

# Generalized linear model
```{r, echo = FALSE, purl = TRUE}
## ---------------------------
## generalized linear model
## ---------------------------

```

To fit a model with binary outcomes, switch to the `glm()`
function. It is set up just like `lm()`, but it has an extra argument,
`family`.[^f1] Set the argument to `binomial()` when your dependent
variable is binary. By default, the `link` function is a
[logit](https://en.wikipedia.org/wiki/Logit) link.

```{r}
## logit
fit <- glm(plan_col_grad ~ bynels2m + as_factor(bypared),
           data = df,
           family = binomial())
summary(fit)

```

If you want a [probit](https://en.wikipedia.org/wiki/Probit_model)
model, just change the link to `probit`. 

```{r}
## probit
fit <- glm(plan_col_grad ~ bynels2m + as_factor(bypared),
           data = df,
           family = binomial(link = 'probit'))
summary(fit)

```

> #### Quick exercise
> Fit a logit or probit model to another binary outcome.

# Using survey weights
```{r, echo = FALSE, purl = TRUE}
## ---------------------------
## survey weights
## ---------------------------

```

So far we haven't used survey weights, but they are very important
when using survey data. To use survey weights load (and install if you
haven't already) the
[survey](http://r-survey.r-forge.r-project.org/survey/) package.

```{r, message = F, warning = F}
## survey library
library(survey)

```

To use survey weights, you need to set the survey design using the
`svydesign()` function. You could do this in the `svyglm()` function
we'll use to actually estimate the equation, but it's easier and
clearer to do it first, store it in an object, and then use that
object in the `syvglm()`.

ELS has a complex sampling design that we won't get into, but the
appropriate columns from our data frame, `df`, are set to the proper
arguments in `svydesign()`:

* `ids` are the primary sampling units or `psu`s  
* `strata` are indicated by the `strat_id`s  
* `weight` is the base-year student weight or `bystuwt`
* `data` is our data frame object, `df`
* `nest = TRUE` because the `psu`s are nested in `strat_id`s

Finally, notice the `~` before each column name, which is necessary in
this function.

```{r}
## set svy design data
svy_df <- svydesign(ids = ~psu,
                    strata = ~strat_id,
                    weight = ~bystuwt,
                    data = df,
                    nest = TRUE)

```

Now that we've set the survey design, we'll use the object `svy_df` in
the `design` argument below (where your data would go in a normal
`lm()` function).

```{r}
## fit the svyglm regression and show output
svyfit <- svyglm(bynels2m ~ byses1 + female + moth_ba + fath_ba + lowinc,
                 design = svy_df)
summary(svyfit)

```
The resulting estimates are survey weighted. The survey library has a
ton of features and is worth diving into if you regularly work with
survey data.

## Predictions
```{r, echo = FALSE, purl = TRUE}
## ------------
## predictions
## ------------

```

Being able to generate predictions from new data can be a powerful
tool. Above, we were able to return the predicted values from the fit
object. We can also use the `predict()` function, however, to return
the standard error of the prediction as well as make predictions for
new observations.

First, we'll get predicted values using the original data along with
their standard errors.

```{r}
## predict from first model
fit <- lm(bynels2m ~ byses1 + female + moth_ba + fath_ba + lowinc,
          data = df)

## old data
fit_pred <- predict(fit, se.fit = TRUE)

## show options
names(fit_pred)
head(fit_pred$fit)
head(fit_pred$se.fit)

```

Ideally, we would have a new observations with which to make
predictions. Then we could test our modeling choices by seeing how
well they predicted new observations. 

With discrete outcomes (like binary 0/1 data), for example, we could
use our model and right-hand side variables from new observations to
predict whether the new observation should have a 0 or 1
outcome. Then, we could compare those predictions to the actual
observed outcomes by making a 2 by
2 [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix)
that counted the numbers of true positives and negatives (correct
predictions) and false positives and negatives (incorrect
predictions). 

In the absence of new data, we instead could have separated our data
into two data sets,
a
[training set and test set](https://en.wikipedia.org/wiki/Training,_test,_and_validation_sets). After
fitting our model to the training data, we could have tested it by
following the above procedure with the testing data. Setting a rule
for ourselves, we could evaluate how well we did, that is, how well
our training data model classified test data outcomes, and perhaps
decide to adjust our modeling assumptions.

Whether making making predictions for new observations or working
within a training/testing data framework, the `predict()` function is
a very useful tool.

## Margins

Using the `predict()` function alongside some other skills we have
practiced, we can also make predictions on the margin a la
Stata's
[`-margins-` suite of commands](https://www.stata.com/help.cgi?margins).

For example, after fitting our multiple regression, we might ask
ourselves, what is the marginal "effect" of having a low income on
math scores, holding all other terms in our model constant? Thinking
about the situation more abstractly, what we are trying to imagine is
two worlds in which the same student lived, identical in all ways
except that in one world the student had a family income below $25,000
a year and in the other did not. If we can make predictions based on
these two students, any difference in predicted math score should be
attributed to the marginal "effect" of having a low family
income.[^f2]

To do this, we need to make a "new" data set with only two rows. For
`lowinc`, each row takes one of the two potential values, 0 and 1. All
other columns in both rows take a consistent value. Usually, this is
the average of the other columns in the model matrix. Though we could
use the full data frame to generate the averages of the other
variables, they may represent different data from what was used to fit
the model if there were missing values that `lm()` dropped. For
consistency, I think it's easier to use the design matrix that's
retrieved from `model.matrix()`.

The code below goes step-by-step to make this new data frame.

```{r}
## create new data that has two rows, with averages and one marginal change

## (1) save model matrix
mm <- model.matrix(fit)
head(mm)

## (2) drop intercept column of ones (predict() doesn't need them)
mm <- mm[,-1]
head(mm)

## (3) convert to data frame so we can use $ notation in next step
mm <- as.data.frame(mm)

## (4) new data frame of means where only lowinc changes
new_df <- data.frame(byses1 = mean(mm$byses1),
                     female = mean(mm$female),
                     moth_ba = mean(mm$moth_ba),
                     fath_ba = mean(mm$fath_ba),
                     lowinc = c(0,1))

## see new data
new_df

```
Notice how the new data frame has the same terms that were used in the
original model, but has only two rows. In the `lowinc` column, the
values switch from 0 to 1. All the other rows are averages of the
data used to fit the model.

To generate the prediction, we use the same function call as before,
but use our `new_df` object with the `newdata` argument.

```{r}
## predict margins
predict(fit, newdata = new_df, se.fit = TRUE)

```

Our results show that compared to otherwise similar students, those
with a family income less than $25,000 a year are predicted to score
about two points lower on their math test.

# Notes

[^f1]: The `lm()` function is just a shorthand convenience function
    for `glm()` in which `family` is set to `gaussian(link =
    'identity')`.
[^f2]: Broadly, this thought experiment falls under
    the
    [potential outcomes framework or Rubin causal model](https://en.wikipedia.org/wiki/Rubin_causal_model). Unless
    our research design is experimental or quasi-experimental, however
    (which ours in this example was not), it is highly unlikely that
    our result will be strictly causal. Since the word *effect* is
    usually reserved to describe causal results, I use scare quotes
    around it to indicate that it shouldn't be interpreted causally
    here.

```{r, echo = FALSE, purl = TRUE}

## =============================================================================
## END SCRIPT
################################################################################
```
